{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12_Convolutional_Network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMwVI1sZss8y+FtZn3gmE1g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mspatke/Deep-Learning-with-Pytorch/blob/main/12_Convolutional_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vq9VzmEqYe93"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transform\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Device Configuration\n",
        "\n",
        "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "7o9RUIMCZARw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBdpc1dUZSnV",
        "outputId": "aef74915-85a2-4123-cf03-a14379f0021a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyper parameters\n",
        "\n",
        "input_size = 784 # 28*28\n",
        "hidden_size = 500\n",
        "num_classes = 10\n",
        "num_epochs = 2\n",
        "batch_size = 2\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "UnBp-TmPaTSw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MNIST Dataset\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root= './',\n",
        "                                           train = True ,\n",
        "                                           transform= transform.ToTensor(),\n",
        "                                           download=True)"
      ],
      "metadata": {
        "id": "11cpdweZamZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root= './',\n",
        "                                          train = False ,\n",
        "                                          transform= transform.ToTensor())"
      ],
      "metadata": {
        "id": "R_13vIS0a7SA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data loader\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset= train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle = True)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset= test_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle = True)"
      ],
      "metadata": {
        "id": "-jMwpUNdbcTo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = iter(test_loader)\n",
        "\n",
        "example_data , example_targets = examples.next()"
      ],
      "metadata": {
        "id": "KhU8HxW2buNS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(example_data[0][0], cmap ='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "lIY_YW58cIgf",
        "outputId": "06b5baba-c7c7-40c0-e7dc-750634f27ff4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f37ad3ca390>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(6):\n",
        "#   plt.subplot(2,3,i+1)\n",
        "#   plt.imshow(example_data[0, 0], cmap='gray')\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "M93ZNZStckzi",
        "outputId": "59ff5179-75df-4635-bcf4-1e6cadf9fd0e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fully connected neural Network with one hideen layer:\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "  def __init__ (self, input_size, hidden_size, num_classes):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.l1 = nn.Linear(input_size , hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.l2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.l2(out)\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "6_NuSTAKdGyA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNet(input_size , hidden_size , num_classes).to(device)"
      ],
      "metadata": {
        "id": "5WEwZbf4fyL6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss and Optimizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= learning_rate)"
      ],
      "metadata": {
        "id": "aClvUyKGf7w2"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    # origin shape: [100, 1,28, 28]\n",
        "    # resized: [100, 784]\n",
        "    images = images.reshape(-1,28*28).to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    #forward pass\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    #backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if(i+1)%100 ==0:\n",
        "      print(f'Epoch[{epoch+1}/{num_epochs}], step [{i+1}/{n_total_steps}], Loss:{loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3RleBLxgKT4",
        "outputId": "33ae524f-6a8d-43ab-89da-0310a97d34fa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[1/2], step [100/30000], Loss:2.3704\n",
            "Epoch[1/2], step [200/30000], Loss:0.9116\n",
            "Epoch[1/2], step [300/30000], Loss:1.8055\n",
            "Epoch[1/2], step [400/30000], Loss:0.0698\n",
            "Epoch[1/2], step [500/30000], Loss:0.1023\n",
            "Epoch[1/2], step [600/30000], Loss:2.7192\n",
            "Epoch[1/2], step [700/30000], Loss:0.4737\n",
            "Epoch[1/2], step [800/30000], Loss:0.5527\n",
            "Epoch[1/2], step [900/30000], Loss:0.0427\n",
            "Epoch[1/2], step [1000/30000], Loss:0.9278\n",
            "Epoch[1/2], step [1100/30000], Loss:1.8450\n",
            "Epoch[1/2], step [1200/30000], Loss:0.0319\n",
            "Epoch[1/2], step [1300/30000], Loss:0.0438\n",
            "Epoch[1/2], step [1400/30000], Loss:0.4889\n",
            "Epoch[1/2], step [1500/30000], Loss:0.0421\n",
            "Epoch[1/2], step [1600/30000], Loss:0.0073\n",
            "Epoch[1/2], step [1700/30000], Loss:0.0692\n",
            "Epoch[1/2], step [1800/30000], Loss:0.0022\n",
            "Epoch[1/2], step [1900/30000], Loss:0.6100\n",
            "Epoch[1/2], step [2000/30000], Loss:0.0126\n",
            "Epoch[1/2], step [2100/30000], Loss:0.0235\n",
            "Epoch[1/2], step [2200/30000], Loss:0.0081\n",
            "Epoch[1/2], step [2300/30000], Loss:0.0373\n",
            "Epoch[1/2], step [2400/30000], Loss:0.0682\n",
            "Epoch[1/2], step [2500/30000], Loss:0.1455\n",
            "Epoch[1/2], step [2600/30000], Loss:0.0336\n",
            "Epoch[1/2], step [2700/30000], Loss:0.2602\n",
            "Epoch[1/2], step [2800/30000], Loss:0.0689\n",
            "Epoch[1/2], step [2900/30000], Loss:0.0492\n",
            "Epoch[1/2], step [3000/30000], Loss:0.0084\n",
            "Epoch[1/2], step [3100/30000], Loss:0.4637\n",
            "Epoch[1/2], step [3200/30000], Loss:0.0018\n",
            "Epoch[1/2], step [3300/30000], Loss:0.8072\n",
            "Epoch[1/2], step [3400/30000], Loss:1.4089\n",
            "Epoch[1/2], step [3500/30000], Loss:0.0661\n",
            "Epoch[1/2], step [3600/30000], Loss:0.0103\n",
            "Epoch[1/2], step [3700/30000], Loss:0.0145\n",
            "Epoch[1/2], step [3800/30000], Loss:0.0017\n",
            "Epoch[1/2], step [3900/30000], Loss:0.4802\n",
            "Epoch[1/2], step [4000/30000], Loss:0.0184\n",
            "Epoch[1/2], step [4100/30000], Loss:0.0103\n",
            "Epoch[1/2], step [4200/30000], Loss:1.1460\n",
            "Epoch[1/2], step [4300/30000], Loss:0.0875\n",
            "Epoch[1/2], step [4400/30000], Loss:0.0127\n",
            "Epoch[1/2], step [4500/30000], Loss:0.0001\n",
            "Epoch[1/2], step [4600/30000], Loss:0.3114\n",
            "Epoch[1/2], step [4700/30000], Loss:0.0867\n",
            "Epoch[1/2], step [4800/30000], Loss:0.0446\n",
            "Epoch[1/2], step [4900/30000], Loss:2.5131\n",
            "Epoch[1/2], step [5000/30000], Loss:0.1374\n",
            "Epoch[1/2], step [5100/30000], Loss:0.0337\n",
            "Epoch[1/2], step [5200/30000], Loss:0.0436\n",
            "Epoch[1/2], step [5300/30000], Loss:0.7385\n",
            "Epoch[1/2], step [5400/30000], Loss:0.0005\n",
            "Epoch[1/2], step [5500/30000], Loss:0.0582\n",
            "Epoch[1/2], step [5600/30000], Loss:0.0153\n",
            "Epoch[1/2], step [5700/30000], Loss:1.7540\n",
            "Epoch[1/2], step [5800/30000], Loss:0.0630\n",
            "Epoch[1/2], step [5900/30000], Loss:0.0034\n",
            "Epoch[1/2], step [6000/30000], Loss:0.0007\n",
            "Epoch[1/2], step [6100/30000], Loss:1.4801\n",
            "Epoch[1/2], step [6200/30000], Loss:0.2171\n",
            "Epoch[1/2], step [6300/30000], Loss:0.0037\n",
            "Epoch[1/2], step [6400/30000], Loss:0.2268\n",
            "Epoch[1/2], step [6500/30000], Loss:0.0004\n",
            "Epoch[1/2], step [6600/30000], Loss:0.0001\n",
            "Epoch[1/2], step [6700/30000], Loss:0.0018\n",
            "Epoch[1/2], step [6800/30000], Loss:0.0014\n",
            "Epoch[1/2], step [6900/30000], Loss:1.5293\n",
            "Epoch[1/2], step [7000/30000], Loss:0.1351\n",
            "Epoch[1/2], step [7100/30000], Loss:0.0570\n",
            "Epoch[1/2], step [7200/30000], Loss:0.0192\n",
            "Epoch[1/2], step [7300/30000], Loss:0.0014\n",
            "Epoch[1/2], step [7400/30000], Loss:0.0370\n",
            "Epoch[1/2], step [7500/30000], Loss:0.3126\n",
            "Epoch[1/2], step [7600/30000], Loss:0.7554\n",
            "Epoch[1/2], step [7700/30000], Loss:0.0466\n",
            "Epoch[1/2], step [7800/30000], Loss:0.0068\n",
            "Epoch[1/2], step [7900/30000], Loss:0.0252\n",
            "Epoch[1/2], step [8000/30000], Loss:0.0037\n",
            "Epoch[1/2], step [8100/30000], Loss:0.0447\n",
            "Epoch[1/2], step [8200/30000], Loss:0.0094\n",
            "Epoch[1/2], step [8300/30000], Loss:0.7141\n",
            "Epoch[1/2], step [8400/30000], Loss:0.0055\n",
            "Epoch[1/2], step [8500/30000], Loss:0.0001\n",
            "Epoch[1/2], step [8600/30000], Loss:0.3016\n",
            "Epoch[1/2], step [8700/30000], Loss:0.0002\n",
            "Epoch[1/2], step [8800/30000], Loss:0.0016\n",
            "Epoch[1/2], step [8900/30000], Loss:0.0001\n",
            "Epoch[1/2], step [9000/30000], Loss:0.0032\n",
            "Epoch[1/2], step [9100/30000], Loss:0.0000\n",
            "Epoch[1/2], step [9200/30000], Loss:0.0002\n",
            "Epoch[1/2], step [9300/30000], Loss:0.0005\n",
            "Epoch[1/2], step [9400/30000], Loss:0.1528\n",
            "Epoch[1/2], step [9500/30000], Loss:0.0003\n",
            "Epoch[1/2], step [9600/30000], Loss:0.0027\n",
            "Epoch[1/2], step [9700/30000], Loss:0.8024\n",
            "Epoch[1/2], step [9800/30000], Loss:0.0047\n",
            "Epoch[1/2], step [9900/30000], Loss:0.0061\n",
            "Epoch[1/2], step [10000/30000], Loss:0.0203\n",
            "Epoch[1/2], step [10100/30000], Loss:0.2055\n",
            "Epoch[1/2], step [10200/30000], Loss:0.0043\n",
            "Epoch[1/2], step [10300/30000], Loss:0.0041\n",
            "Epoch[1/2], step [10400/30000], Loss:0.0012\n",
            "Epoch[1/2], step [10500/30000], Loss:0.0015\n",
            "Epoch[1/2], step [10600/30000], Loss:0.0004\n",
            "Epoch[1/2], step [10700/30000], Loss:0.0090\n",
            "Epoch[1/2], step [10800/30000], Loss:0.0039\n",
            "Epoch[1/2], step [10900/30000], Loss:0.0044\n",
            "Epoch[1/2], step [11000/30000], Loss:0.0013\n",
            "Epoch[1/2], step [11100/30000], Loss:0.0003\n",
            "Epoch[1/2], step [11200/30000], Loss:0.0002\n",
            "Epoch[1/2], step [11300/30000], Loss:0.0065\n",
            "Epoch[1/2], step [11400/30000], Loss:0.0372\n",
            "Epoch[1/2], step [11500/30000], Loss:0.1387\n",
            "Epoch[1/2], step [11600/30000], Loss:0.8442\n",
            "Epoch[1/2], step [11700/30000], Loss:0.0002\n",
            "Epoch[1/2], step [11800/30000], Loss:0.0000\n",
            "Epoch[1/2], step [11900/30000], Loss:0.0688\n",
            "Epoch[1/2], step [12000/30000], Loss:0.0271\n",
            "Epoch[1/2], step [12100/30000], Loss:0.0758\n",
            "Epoch[1/2], step [12200/30000], Loss:0.1057\n",
            "Epoch[1/2], step [12300/30000], Loss:0.0041\n",
            "Epoch[1/2], step [12400/30000], Loss:0.0032\n",
            "Epoch[1/2], step [12500/30000], Loss:0.4874\n",
            "Epoch[1/2], step [12600/30000], Loss:0.0420\n",
            "Epoch[1/2], step [12700/30000], Loss:0.2801\n",
            "Epoch[1/2], step [12800/30000], Loss:0.0715\n",
            "Epoch[1/2], step [12900/30000], Loss:0.1670\n",
            "Epoch[1/2], step [13000/30000], Loss:0.0007\n",
            "Epoch[1/2], step [13100/30000], Loss:0.0003\n",
            "Epoch[1/2], step [13200/30000], Loss:0.0012\n",
            "Epoch[1/2], step [13300/30000], Loss:0.0009\n",
            "Epoch[1/2], step [13400/30000], Loss:0.0020\n",
            "Epoch[1/2], step [13500/30000], Loss:0.0025\n",
            "Epoch[1/2], step [13600/30000], Loss:0.0000\n",
            "Epoch[1/2], step [13700/30000], Loss:0.0037\n",
            "Epoch[1/2], step [13800/30000], Loss:0.0021\n",
            "Epoch[1/2], step [13900/30000], Loss:0.0094\n",
            "Epoch[1/2], step [14000/30000], Loss:0.0097\n",
            "Epoch[1/2], step [14100/30000], Loss:0.0016\n",
            "Epoch[1/2], step [14200/30000], Loss:0.2411\n",
            "Epoch[1/2], step [14300/30000], Loss:0.0000\n",
            "Epoch[1/2], step [14400/30000], Loss:0.0043\n",
            "Epoch[1/2], step [14500/30000], Loss:0.1846\n",
            "Epoch[1/2], step [14600/30000], Loss:0.0003\n",
            "Epoch[1/2], step [14700/30000], Loss:0.0000\n",
            "Epoch[1/2], step [14800/30000], Loss:0.0001\n",
            "Epoch[1/2], step [14900/30000], Loss:1.2830\n",
            "Epoch[1/2], step [15000/30000], Loss:0.0175\n",
            "Epoch[1/2], step [15100/30000], Loss:0.0109\n",
            "Epoch[1/2], step [15200/30000], Loss:0.0007\n",
            "Epoch[1/2], step [15300/30000], Loss:0.0000\n",
            "Epoch[1/2], step [15400/30000], Loss:0.0004\n",
            "Epoch[1/2], step [15500/30000], Loss:0.0003\n",
            "Epoch[1/2], step [15600/30000], Loss:1.1947\n",
            "Epoch[1/2], step [15700/30000], Loss:0.0268\n",
            "Epoch[1/2], step [15800/30000], Loss:0.0031\n",
            "Epoch[1/2], step [15900/30000], Loss:0.8299\n",
            "Epoch[1/2], step [16000/30000], Loss:0.0532\n",
            "Epoch[1/2], step [16100/30000], Loss:0.0056\n",
            "Epoch[1/2], step [16200/30000], Loss:0.0071\n",
            "Epoch[1/2], step [16300/30000], Loss:0.0014\n",
            "Epoch[1/2], step [16400/30000], Loss:0.4970\n",
            "Epoch[1/2], step [16500/30000], Loss:0.0003\n",
            "Epoch[1/2], step [16600/30000], Loss:0.0000\n",
            "Epoch[1/2], step [16700/30000], Loss:0.8459\n",
            "Epoch[1/2], step [16800/30000], Loss:0.0000\n",
            "Epoch[1/2], step [16900/30000], Loss:0.0034\n",
            "Epoch[1/2], step [17000/30000], Loss:0.3908\n",
            "Epoch[1/2], step [17100/30000], Loss:0.0000\n",
            "Epoch[1/2], step [17200/30000], Loss:0.0105\n",
            "Epoch[1/2], step [17300/30000], Loss:0.0008\n",
            "Epoch[1/2], step [17400/30000], Loss:0.0316\n",
            "Epoch[1/2], step [17500/30000], Loss:0.0003\n",
            "Epoch[1/2], step [17600/30000], Loss:0.0013\n",
            "Epoch[1/2], step [17700/30000], Loss:0.0003\n",
            "Epoch[1/2], step [17800/30000], Loss:0.0002\n",
            "Epoch[1/2], step [17900/30000], Loss:0.0003\n",
            "Epoch[1/2], step [18000/30000], Loss:0.0002\n",
            "Epoch[1/2], step [18100/30000], Loss:0.0001\n",
            "Epoch[1/2], step [18200/30000], Loss:3.7565\n",
            "Epoch[1/2], step [18300/30000], Loss:0.0089\n",
            "Epoch[1/2], step [18400/30000], Loss:1.5730\n",
            "Epoch[1/2], step [18500/30000], Loss:0.0003\n",
            "Epoch[1/2], step [18600/30000], Loss:0.0020\n",
            "Epoch[1/2], step [18700/30000], Loss:0.0005\n",
            "Epoch[1/2], step [18800/30000], Loss:0.0010\n",
            "Epoch[1/2], step [18900/30000], Loss:0.0732\n",
            "Epoch[1/2], step [19000/30000], Loss:0.0694\n",
            "Epoch[1/2], step [19100/30000], Loss:0.0001\n",
            "Epoch[1/2], step [19200/30000], Loss:0.0061\n",
            "Epoch[1/2], step [19300/30000], Loss:0.0040\n",
            "Epoch[1/2], step [19400/30000], Loss:0.0409\n",
            "Epoch[1/2], step [19500/30000], Loss:0.0158\n",
            "Epoch[1/2], step [19600/30000], Loss:0.0026\n",
            "Epoch[1/2], step [19700/30000], Loss:0.0001\n",
            "Epoch[1/2], step [19800/30000], Loss:0.1264\n",
            "Epoch[1/2], step [19900/30000], Loss:0.0003\n",
            "Epoch[1/2], step [20000/30000], Loss:0.0594\n",
            "Epoch[1/2], step [20100/30000], Loss:0.0005\n",
            "Epoch[1/2], step [20200/30000], Loss:0.0000\n",
            "Epoch[1/2], step [20300/30000], Loss:0.0006\n",
            "Epoch[1/2], step [20400/30000], Loss:0.0000\n",
            "Epoch[1/2], step [20500/30000], Loss:0.0004\n",
            "Epoch[1/2], step [20600/30000], Loss:0.0000\n",
            "Epoch[1/2], step [20700/30000], Loss:0.0131\n",
            "Epoch[1/2], step [20800/30000], Loss:0.0145\n",
            "Epoch[1/2], step [20900/30000], Loss:0.0378\n",
            "Epoch[1/2], step [21000/30000], Loss:0.0002\n",
            "Epoch[1/2], step [21100/30000], Loss:0.0001\n",
            "Epoch[1/2], step [21200/30000], Loss:0.0217\n",
            "Epoch[1/2], step [21300/30000], Loss:0.1954\n",
            "Epoch[1/2], step [21400/30000], Loss:0.0000\n",
            "Epoch[1/2], step [21500/30000], Loss:0.0006\n",
            "Epoch[1/2], step [21600/30000], Loss:0.0015\n",
            "Epoch[1/2], step [21700/30000], Loss:0.0008\n",
            "Epoch[1/2], step [21800/30000], Loss:0.0154\n",
            "Epoch[1/2], step [21900/30000], Loss:0.0014\n",
            "Epoch[1/2], step [22000/30000], Loss:0.0055\n",
            "Epoch[1/2], step [22100/30000], Loss:0.0190\n",
            "Epoch[1/2], step [22200/30000], Loss:0.0007\n",
            "Epoch[1/2], step [22300/30000], Loss:0.0001\n",
            "Epoch[1/2], step [22400/30000], Loss:0.0000\n",
            "Epoch[1/2], step [22500/30000], Loss:0.0086\n",
            "Epoch[1/2], step [22600/30000], Loss:0.0158\n",
            "Epoch[1/2], step [22700/30000], Loss:0.0003\n",
            "Epoch[1/2], step [22800/30000], Loss:0.0000\n",
            "Epoch[1/2], step [22900/30000], Loss:0.0140\n",
            "Epoch[1/2], step [23000/30000], Loss:0.0064\n",
            "Epoch[1/2], step [23100/30000], Loss:0.0031\n",
            "Epoch[1/2], step [23200/30000], Loss:0.0000\n",
            "Epoch[1/2], step [23300/30000], Loss:0.0001\n",
            "Epoch[1/2], step [23400/30000], Loss:0.0006\n",
            "Epoch[1/2], step [23500/30000], Loss:0.0000\n",
            "Epoch[1/2], step [23600/30000], Loss:0.0004\n",
            "Epoch[1/2], step [23700/30000], Loss:0.0000\n",
            "Epoch[1/2], step [23800/30000], Loss:0.0059\n",
            "Epoch[1/2], step [23900/30000], Loss:0.0099\n",
            "Epoch[1/2], step [24000/30000], Loss:0.0000\n",
            "Epoch[1/2], step [24100/30000], Loss:0.0002\n",
            "Epoch[1/2], step [24200/30000], Loss:0.0456\n",
            "Epoch[1/2], step [24300/30000], Loss:0.0040\n",
            "Epoch[1/2], step [24400/30000], Loss:0.1187\n",
            "Epoch[1/2], step [24500/30000], Loss:0.0000\n",
            "Epoch[1/2], step [24600/30000], Loss:0.0419\n",
            "Epoch[1/2], step [24700/30000], Loss:0.0001\n",
            "Epoch[1/2], step [24800/30000], Loss:0.0001\n",
            "Epoch[1/2], step [24900/30000], Loss:0.4352\n",
            "Epoch[1/2], step [25000/30000], Loss:0.0002\n",
            "Epoch[1/2], step [25100/30000], Loss:0.0000\n",
            "Epoch[1/2], step [25200/30000], Loss:0.0201\n",
            "Epoch[1/2], step [25300/30000], Loss:0.1206\n",
            "Epoch[1/2], step [25400/30000], Loss:0.0001\n",
            "Epoch[1/2], step [25500/30000], Loss:0.0006\n",
            "Epoch[1/2], step [25600/30000], Loss:0.0001\n",
            "Epoch[1/2], step [25700/30000], Loss:0.0007\n",
            "Epoch[1/2], step [25800/30000], Loss:0.0000\n",
            "Epoch[1/2], step [25900/30000], Loss:0.0031\n",
            "Epoch[1/2], step [26000/30000], Loss:0.0000\n",
            "Epoch[1/2], step [26100/30000], Loss:0.0014\n",
            "Epoch[1/2], step [26200/30000], Loss:2.0776\n",
            "Epoch[1/2], step [26300/30000], Loss:0.0001\n",
            "Epoch[1/2], step [26400/30000], Loss:0.0003\n",
            "Epoch[1/2], step [26500/30000], Loss:0.0021\n",
            "Epoch[1/2], step [26600/30000], Loss:0.2504\n",
            "Epoch[1/2], step [26700/30000], Loss:0.0000\n",
            "Epoch[1/2], step [26800/30000], Loss:0.0000\n",
            "Epoch[1/2], step [26900/30000], Loss:0.0067\n",
            "Epoch[1/2], step [27000/30000], Loss:0.0397\n",
            "Epoch[1/2], step [27100/30000], Loss:0.0002\n",
            "Epoch[1/2], step [27200/30000], Loss:1.9094\n",
            "Epoch[1/2], step [27300/30000], Loss:0.2325\n",
            "Epoch[1/2], step [27400/30000], Loss:0.0001\n",
            "Epoch[1/2], step [27500/30000], Loss:0.6159\n",
            "Epoch[1/2], step [27600/30000], Loss:0.0000\n",
            "Epoch[1/2], step [27700/30000], Loss:0.2069\n",
            "Epoch[1/2], step [27800/30000], Loss:0.7125\n",
            "Epoch[1/2], step [27900/30000], Loss:0.0182\n",
            "Epoch[1/2], step [28000/30000], Loss:0.0197\n",
            "Epoch[1/2], step [28100/30000], Loss:0.0002\n",
            "Epoch[1/2], step [28200/30000], Loss:0.6648\n",
            "Epoch[1/2], step [28300/30000], Loss:0.0015\n",
            "Epoch[1/2], step [28400/30000], Loss:0.0308\n",
            "Epoch[1/2], step [28500/30000], Loss:0.0061\n",
            "Epoch[1/2], step [28600/30000], Loss:0.0000\n",
            "Epoch[1/2], step [28700/30000], Loss:0.0229\n",
            "Epoch[1/2], step [28800/30000], Loss:0.0177\n",
            "Epoch[1/2], step [28900/30000], Loss:0.0000\n",
            "Epoch[1/2], step [29000/30000], Loss:0.0005\n",
            "Epoch[1/2], step [29100/30000], Loss:0.0002\n",
            "Epoch[1/2], step [29200/30000], Loss:0.0000\n",
            "Epoch[1/2], step [29300/30000], Loss:0.0374\n",
            "Epoch[1/2], step [29400/30000], Loss:0.0000\n",
            "Epoch[1/2], step [29500/30000], Loss:0.0001\n",
            "Epoch[1/2], step [29600/30000], Loss:0.0009\n",
            "Epoch[1/2], step [29700/30000], Loss:0.0004\n",
            "Epoch[1/2], step [29800/30000], Loss:0.0006\n",
            "Epoch[1/2], step [29900/30000], Loss:0.0000\n",
            "Epoch[1/2], step [30000/30000], Loss:0.0070\n",
            "Epoch[2/2], step [100/30000], Loss:0.0000\n",
            "Epoch[2/2], step [200/30000], Loss:0.0312\n",
            "Epoch[2/2], step [300/30000], Loss:0.0562\n",
            "Epoch[2/2], step [400/30000], Loss:0.0000\n",
            "Epoch[2/2], step [500/30000], Loss:0.0199\n",
            "Epoch[2/2], step [600/30000], Loss:0.0141\n",
            "Epoch[2/2], step [700/30000], Loss:0.0021\n",
            "Epoch[2/2], step [800/30000], Loss:0.0683\n",
            "Epoch[2/2], step [900/30000], Loss:0.0000\n",
            "Epoch[2/2], step [1000/30000], Loss:0.0000\n",
            "Epoch[2/2], step [1100/30000], Loss:0.0003\n",
            "Epoch[2/2], step [1200/30000], Loss:0.0000\n",
            "Epoch[2/2], step [1300/30000], Loss:0.0014\n",
            "Epoch[2/2], step [1400/30000], Loss:0.0001\n",
            "Epoch[2/2], step [1500/30000], Loss:0.0072\n",
            "Epoch[2/2], step [1600/30000], Loss:0.2255\n",
            "Epoch[2/2], step [1700/30000], Loss:0.0449\n",
            "Epoch[2/2], step [1800/30000], Loss:0.0001\n",
            "Epoch[2/2], step [1900/30000], Loss:0.0052\n",
            "Epoch[2/2], step [2000/30000], Loss:0.0020\n",
            "Epoch[2/2], step [2100/30000], Loss:0.0088\n",
            "Epoch[2/2], step [2200/30000], Loss:0.0120\n",
            "Epoch[2/2], step [2300/30000], Loss:0.1071\n",
            "Epoch[2/2], step [2400/30000], Loss:0.0008\n",
            "Epoch[2/2], step [2500/30000], Loss:0.0049\n",
            "Epoch[2/2], step [2600/30000], Loss:0.0001\n",
            "Epoch[2/2], step [2700/30000], Loss:0.0203\n",
            "Epoch[2/2], step [2800/30000], Loss:0.0002\n",
            "Epoch[2/2], step [2900/30000], Loss:0.0437\n",
            "Epoch[2/2], step [3000/30000], Loss:0.0016\n",
            "Epoch[2/2], step [3100/30000], Loss:0.0000\n",
            "Epoch[2/2], step [3200/30000], Loss:0.0007\n",
            "Epoch[2/2], step [3300/30000], Loss:0.0000\n",
            "Epoch[2/2], step [3400/30000], Loss:0.0000\n",
            "Epoch[2/2], step [3500/30000], Loss:0.0093\n",
            "Epoch[2/2], step [3600/30000], Loss:0.0041\n",
            "Epoch[2/2], step [3700/30000], Loss:0.0001\n",
            "Epoch[2/2], step [3800/30000], Loss:0.0004\n",
            "Epoch[2/2], step [3900/30000], Loss:0.0010\n",
            "Epoch[2/2], step [4000/30000], Loss:0.0001\n",
            "Epoch[2/2], step [4100/30000], Loss:0.0020\n",
            "Epoch[2/2], step [4200/30000], Loss:0.0000\n",
            "Epoch[2/2], step [4300/30000], Loss:0.0000\n",
            "Epoch[2/2], step [4400/30000], Loss:0.0037\n",
            "Epoch[2/2], step [4500/30000], Loss:0.0000\n",
            "Epoch[2/2], step [4600/30000], Loss:0.0000\n",
            "Epoch[2/2], step [4700/30000], Loss:0.0000\n",
            "Epoch[2/2], step [4800/30000], Loss:0.0008\n",
            "Epoch[2/2], step [4900/30000], Loss:0.0000\n",
            "Epoch[2/2], step [5000/30000], Loss:0.0000\n",
            "Epoch[2/2], step [5100/30000], Loss:0.0000\n",
            "Epoch[2/2], step [5200/30000], Loss:0.0003\n",
            "Epoch[2/2], step [5300/30000], Loss:0.0005\n",
            "Epoch[2/2], step [5400/30000], Loss:0.1982\n",
            "Epoch[2/2], step [5500/30000], Loss:0.0004\n",
            "Epoch[2/2], step [5600/30000], Loss:0.0000\n",
            "Epoch[2/2], step [5700/30000], Loss:0.0023\n",
            "Epoch[2/2], step [5800/30000], Loss:3.3519\n",
            "Epoch[2/2], step [5900/30000], Loss:0.3170\n",
            "Epoch[2/2], step [6000/30000], Loss:0.1225\n",
            "Epoch[2/2], step [6100/30000], Loss:0.0742\n",
            "Epoch[2/2], step [6200/30000], Loss:0.0000\n",
            "Epoch[2/2], step [6300/30000], Loss:0.0036\n",
            "Epoch[2/2], step [6400/30000], Loss:0.0001\n",
            "Epoch[2/2], step [6500/30000], Loss:0.0000\n",
            "Epoch[2/2], step [6600/30000], Loss:0.0000\n",
            "Epoch[2/2], step [6700/30000], Loss:0.0005\n",
            "Epoch[2/2], step [6800/30000], Loss:0.0264\n",
            "Epoch[2/2], step [6900/30000], Loss:0.0000\n",
            "Epoch[2/2], step [7000/30000], Loss:0.0001\n",
            "Epoch[2/2], step [7100/30000], Loss:0.0017\n",
            "Epoch[2/2], step [7200/30000], Loss:0.0110\n",
            "Epoch[2/2], step [7300/30000], Loss:0.0000\n",
            "Epoch[2/2], step [7400/30000], Loss:0.0000\n",
            "Epoch[2/2], step [7500/30000], Loss:0.0000\n",
            "Epoch[2/2], step [7600/30000], Loss:0.0000\n",
            "Epoch[2/2], step [7700/30000], Loss:0.0392\n",
            "Epoch[2/2], step [7800/30000], Loss:0.0000\n",
            "Epoch[2/2], step [7900/30000], Loss:0.5408\n",
            "Epoch[2/2], step [8000/30000], Loss:0.0000\n",
            "Epoch[2/2], step [8100/30000], Loss:0.0000\n",
            "Epoch[2/2], step [8200/30000], Loss:0.0000\n",
            "Epoch[2/2], step [8300/30000], Loss:0.0000\n",
            "Epoch[2/2], step [8400/30000], Loss:0.0000\n",
            "Epoch[2/2], step [8500/30000], Loss:0.0782\n",
            "Epoch[2/2], step [8600/30000], Loss:0.0000\n",
            "Epoch[2/2], step [8700/30000], Loss:0.0007\n",
            "Epoch[2/2], step [8800/30000], Loss:0.0021\n",
            "Epoch[2/2], step [8900/30000], Loss:0.0099\n",
            "Epoch[2/2], step [9000/30000], Loss:0.0115\n",
            "Epoch[2/2], step [9100/30000], Loss:0.0001\n",
            "Epoch[2/2], step [9200/30000], Loss:0.0003\n",
            "Epoch[2/2], step [9300/30000], Loss:0.0000\n",
            "Epoch[2/2], step [9400/30000], Loss:0.0001\n",
            "Epoch[2/2], step [9500/30000], Loss:0.0287\n",
            "Epoch[2/2], step [9600/30000], Loss:0.1250\n",
            "Epoch[2/2], step [9700/30000], Loss:0.0002\n",
            "Epoch[2/2], step [9800/30000], Loss:0.0002\n",
            "Epoch[2/2], step [9900/30000], Loss:0.0135\n",
            "Epoch[2/2], step [10000/30000], Loss:0.0000\n",
            "Epoch[2/2], step [10100/30000], Loss:0.0000\n",
            "Epoch[2/2], step [10200/30000], Loss:0.0082\n",
            "Epoch[2/2], step [10300/30000], Loss:0.0000\n",
            "Epoch[2/2], step [10400/30000], Loss:0.0000\n",
            "Epoch[2/2], step [10500/30000], Loss:0.0000\n",
            "Epoch[2/2], step [10600/30000], Loss:0.0014\n",
            "Epoch[2/2], step [10700/30000], Loss:0.0006\n",
            "Epoch[2/2], step [10800/30000], Loss:0.0001\n",
            "Epoch[2/2], step [10900/30000], Loss:0.0000\n",
            "Epoch[2/2], step [11000/30000], Loss:0.0000\n",
            "Epoch[2/2], step [11100/30000], Loss:0.0000\n",
            "Epoch[2/2], step [11200/30000], Loss:0.0000\n",
            "Epoch[2/2], step [11300/30000], Loss:1.5663\n",
            "Epoch[2/2], step [11400/30000], Loss:0.0029\n",
            "Epoch[2/2], step [11500/30000], Loss:0.0028\n",
            "Epoch[2/2], step [11600/30000], Loss:0.0000\n",
            "Epoch[2/2], step [11700/30000], Loss:0.0000\n",
            "Epoch[2/2], step [11800/30000], Loss:0.0001\n",
            "Epoch[2/2], step [11900/30000], Loss:0.0000\n",
            "Epoch[2/2], step [12000/30000], Loss:0.0560\n",
            "Epoch[2/2], step [12100/30000], Loss:0.0002\n",
            "Epoch[2/2], step [12200/30000], Loss:0.0000\n",
            "Epoch[2/2], step [12300/30000], Loss:0.0663\n",
            "Epoch[2/2], step [12400/30000], Loss:0.0001\n",
            "Epoch[2/2], step [12500/30000], Loss:0.2734\n",
            "Epoch[2/2], step [12600/30000], Loss:0.0000\n",
            "Epoch[2/2], step [12700/30000], Loss:0.0005\n",
            "Epoch[2/2], step [12800/30000], Loss:0.0000\n",
            "Epoch[2/2], step [12900/30000], Loss:0.0001\n",
            "Epoch[2/2], step [13000/30000], Loss:0.0000\n",
            "Epoch[2/2], step [13100/30000], Loss:0.0000\n",
            "Epoch[2/2], step [13200/30000], Loss:0.0990\n",
            "Epoch[2/2], step [13300/30000], Loss:0.0000\n",
            "Epoch[2/2], step [13400/30000], Loss:0.0000\n",
            "Epoch[2/2], step [13500/30000], Loss:0.0000\n",
            "Epoch[2/2], step [13600/30000], Loss:0.9252\n",
            "Epoch[2/2], step [13700/30000], Loss:0.0552\n",
            "Epoch[2/2], step [13800/30000], Loss:0.0008\n",
            "Epoch[2/2], step [13900/30000], Loss:0.0000\n",
            "Epoch[2/2], step [14000/30000], Loss:0.0642\n",
            "Epoch[2/2], step [14100/30000], Loss:0.0005\n",
            "Epoch[2/2], step [14200/30000], Loss:0.3401\n",
            "Epoch[2/2], step [14300/30000], Loss:0.0021\n",
            "Epoch[2/2], step [14400/30000], Loss:4.2824\n",
            "Epoch[2/2], step [14500/30000], Loss:0.1827\n",
            "Epoch[2/2], step [14600/30000], Loss:0.0000\n",
            "Epoch[2/2], step [14700/30000], Loss:0.8687\n",
            "Epoch[2/2], step [14800/30000], Loss:0.0001\n",
            "Epoch[2/2], step [14900/30000], Loss:0.0000\n",
            "Epoch[2/2], step [15000/30000], Loss:0.0040\n",
            "Epoch[2/2], step [15100/30000], Loss:0.0000\n",
            "Epoch[2/2], step [15200/30000], Loss:0.0002\n",
            "Epoch[2/2], step [15300/30000], Loss:0.0089\n",
            "Epoch[2/2], step [15400/30000], Loss:0.0000\n",
            "Epoch[2/2], step [15500/30000], Loss:0.5843\n",
            "Epoch[2/2], step [15600/30000], Loss:0.0001\n",
            "Epoch[2/2], step [15700/30000], Loss:0.0283\n",
            "Epoch[2/2], step [15800/30000], Loss:0.0000\n",
            "Epoch[2/2], step [15900/30000], Loss:0.0000\n",
            "Epoch[2/2], step [16000/30000], Loss:2.1271\n",
            "Epoch[2/2], step [16100/30000], Loss:0.0000\n",
            "Epoch[2/2], step [16200/30000], Loss:0.0003\n",
            "Epoch[2/2], step [16300/30000], Loss:0.0904\n",
            "Epoch[2/2], step [16400/30000], Loss:0.0000\n",
            "Epoch[2/2], step [16500/30000], Loss:0.0261\n",
            "Epoch[2/2], step [16600/30000], Loss:2.2515\n",
            "Epoch[2/2], step [16700/30000], Loss:0.0792\n",
            "Epoch[2/2], step [16800/30000], Loss:0.0000\n",
            "Epoch[2/2], step [16900/30000], Loss:0.0000\n",
            "Epoch[2/2], step [17000/30000], Loss:0.0001\n",
            "Epoch[2/2], step [17100/30000], Loss:0.0003\n",
            "Epoch[2/2], step [17200/30000], Loss:0.0027\n",
            "Epoch[2/2], step [17300/30000], Loss:0.0000\n",
            "Epoch[2/2], step [17400/30000], Loss:0.0000\n",
            "Epoch[2/2], step [17500/30000], Loss:0.0000\n",
            "Epoch[2/2], step [17600/30000], Loss:0.0000\n",
            "Epoch[2/2], step [17700/30000], Loss:0.0000\n",
            "Epoch[2/2], step [17800/30000], Loss:0.0001\n",
            "Epoch[2/2], step [17900/30000], Loss:0.0000\n",
            "Epoch[2/2], step [18000/30000], Loss:0.0051\n",
            "Epoch[2/2], step [18100/30000], Loss:0.0000\n",
            "Epoch[2/2], step [18200/30000], Loss:0.0001\n",
            "Epoch[2/2], step [18300/30000], Loss:0.0000\n",
            "Epoch[2/2], step [18400/30000], Loss:0.0000\n",
            "Epoch[2/2], step [18500/30000], Loss:0.0000\n",
            "Epoch[2/2], step [18600/30000], Loss:0.0000\n",
            "Epoch[2/2], step [18700/30000], Loss:0.0000\n",
            "Epoch[2/2], step [18800/30000], Loss:0.5171\n",
            "Epoch[2/2], step [18900/30000], Loss:0.0000\n",
            "Epoch[2/2], step [19000/30000], Loss:0.0001\n",
            "Epoch[2/2], step [19100/30000], Loss:0.0000\n",
            "Epoch[2/2], step [19200/30000], Loss:0.1961\n",
            "Epoch[2/2], step [19300/30000], Loss:0.0002\n",
            "Epoch[2/2], step [19400/30000], Loss:0.0002\n",
            "Epoch[2/2], step [19500/30000], Loss:0.0000\n",
            "Epoch[2/2], step [19600/30000], Loss:0.0003\n",
            "Epoch[2/2], step [19700/30000], Loss:0.0000\n",
            "Epoch[2/2], step [19800/30000], Loss:0.0000\n",
            "Epoch[2/2], step [19900/30000], Loss:0.0002\n",
            "Epoch[2/2], step [20000/30000], Loss:0.0000\n",
            "Epoch[2/2], step [20100/30000], Loss:0.0002\n",
            "Epoch[2/2], step [20200/30000], Loss:0.0597\n",
            "Epoch[2/2], step [20300/30000], Loss:0.0004\n",
            "Epoch[2/2], step [20400/30000], Loss:0.0000\n",
            "Epoch[2/2], step [20500/30000], Loss:0.0008\n",
            "Epoch[2/2], step [20600/30000], Loss:1.0504\n",
            "Epoch[2/2], step [20700/30000], Loss:0.0244\n",
            "Epoch[2/2], step [20800/30000], Loss:0.0135\n",
            "Epoch[2/2], step [20900/30000], Loss:0.0139\n",
            "Epoch[2/2], step [21000/30000], Loss:0.0043\n",
            "Epoch[2/2], step [21100/30000], Loss:0.0001\n",
            "Epoch[2/2], step [21200/30000], Loss:0.0000\n",
            "Epoch[2/2], step [21300/30000], Loss:0.0000\n",
            "Epoch[2/2], step [21400/30000], Loss:0.0000\n",
            "Epoch[2/2], step [21500/30000], Loss:0.0000\n",
            "Epoch[2/2], step [21600/30000], Loss:0.0001\n",
            "Epoch[2/2], step [21700/30000], Loss:0.0000\n",
            "Epoch[2/2], step [21800/30000], Loss:0.0000\n",
            "Epoch[2/2], step [21900/30000], Loss:0.0000\n",
            "Epoch[2/2], step [22000/30000], Loss:0.0000\n",
            "Epoch[2/2], step [22100/30000], Loss:0.0096\n",
            "Epoch[2/2], step [22200/30000], Loss:0.0189\n",
            "Epoch[2/2], step [22300/30000], Loss:0.0414\n",
            "Epoch[2/2], step [22400/30000], Loss:0.0024\n",
            "Epoch[2/2], step [22500/30000], Loss:0.0001\n",
            "Epoch[2/2], step [22600/30000], Loss:0.0009\n",
            "Epoch[2/2], step [22700/30000], Loss:0.0002\n",
            "Epoch[2/2], step [22800/30000], Loss:0.0008\n",
            "Epoch[2/2], step [22900/30000], Loss:0.0002\n",
            "Epoch[2/2], step [23000/30000], Loss:0.0003\n",
            "Epoch[2/2], step [23100/30000], Loss:0.0001\n",
            "Epoch[2/2], step [23200/30000], Loss:0.0000\n",
            "Epoch[2/2], step [23300/30000], Loss:0.0000\n",
            "Epoch[2/2], step [23400/30000], Loss:0.0000\n",
            "Epoch[2/2], step [23500/30000], Loss:0.0000\n",
            "Epoch[2/2], step [23600/30000], Loss:0.0106\n",
            "Epoch[2/2], step [23700/30000], Loss:0.1136\n",
            "Epoch[2/2], step [23800/30000], Loss:0.0018\n",
            "Epoch[2/2], step [23900/30000], Loss:0.0000\n",
            "Epoch[2/2], step [24000/30000], Loss:0.0075\n",
            "Epoch[2/2], step [24100/30000], Loss:0.0000\n",
            "Epoch[2/2], step [24200/30000], Loss:0.0005\n",
            "Epoch[2/2], step [24300/30000], Loss:0.0346\n",
            "Epoch[2/2], step [24400/30000], Loss:2.9437\n",
            "Epoch[2/2], step [24500/30000], Loss:0.0001\n",
            "Epoch[2/2], step [24600/30000], Loss:2.1845\n",
            "Epoch[2/2], step [24700/30000], Loss:0.0069\n",
            "Epoch[2/2], step [24800/30000], Loss:0.0006\n",
            "Epoch[2/2], step [24900/30000], Loss:0.3068\n",
            "Epoch[2/2], step [25000/30000], Loss:0.0428\n",
            "Epoch[2/2], step [25100/30000], Loss:0.0005\n",
            "Epoch[2/2], step [25200/30000], Loss:0.0000\n",
            "Epoch[2/2], step [25300/30000], Loss:0.0000\n",
            "Epoch[2/2], step [25400/30000], Loss:0.3044\n",
            "Epoch[2/2], step [25500/30000], Loss:0.0000\n",
            "Epoch[2/2], step [25600/30000], Loss:0.0000\n",
            "Epoch[2/2], step [25700/30000], Loss:0.0002\n",
            "Epoch[2/2], step [25800/30000], Loss:0.0910\n",
            "Epoch[2/2], step [25900/30000], Loss:0.0001\n",
            "Epoch[2/2], step [26000/30000], Loss:0.0000\n",
            "Epoch[2/2], step [26100/30000], Loss:0.0052\n",
            "Epoch[2/2], step [26200/30000], Loss:0.0000\n",
            "Epoch[2/2], step [26300/30000], Loss:0.0000\n",
            "Epoch[2/2], step [26400/30000], Loss:0.0008\n",
            "Epoch[2/2], step [26500/30000], Loss:0.0001\n",
            "Epoch[2/2], step [26600/30000], Loss:0.0855\n",
            "Epoch[2/2], step [26700/30000], Loss:0.0000\n",
            "Epoch[2/2], step [26800/30000], Loss:0.0370\n",
            "Epoch[2/2], step [26900/30000], Loss:0.0000\n",
            "Epoch[2/2], step [27000/30000], Loss:0.0000\n",
            "Epoch[2/2], step [27100/30000], Loss:0.0000\n",
            "Epoch[2/2], step [27200/30000], Loss:0.1904\n",
            "Epoch[2/2], step [27300/30000], Loss:0.0000\n",
            "Epoch[2/2], step [27400/30000], Loss:0.0001\n",
            "Epoch[2/2], step [27500/30000], Loss:0.0005\n",
            "Epoch[2/2], step [27600/30000], Loss:0.0004\n",
            "Epoch[2/2], step [27700/30000], Loss:0.0001\n",
            "Epoch[2/2], step [27800/30000], Loss:0.0003\n",
            "Epoch[2/2], step [27900/30000], Loss:0.0000\n",
            "Epoch[2/2], step [28000/30000], Loss:0.0019\n",
            "Epoch[2/2], step [28100/30000], Loss:0.0008\n",
            "Epoch[2/2], step [28200/30000], Loss:0.0030\n",
            "Epoch[2/2], step [28300/30000], Loss:0.0188\n",
            "Epoch[2/2], step [28400/30000], Loss:0.0000\n",
            "Epoch[2/2], step [28500/30000], Loss:0.0000\n",
            "Epoch[2/2], step [28600/30000], Loss:0.0000\n",
            "Epoch[2/2], step [28700/30000], Loss:0.0000\n",
            "Epoch[2/2], step [28800/30000], Loss:0.0016\n",
            "Epoch[2/2], step [28900/30000], Loss:0.0000\n",
            "Epoch[2/2], step [29000/30000], Loss:0.0038\n",
            "Epoch[2/2], step [29100/30000], Loss:0.0199\n",
            "Epoch[2/2], step [29200/30000], Loss:0.0091\n",
            "Epoch[2/2], step [29300/30000], Loss:0.0000\n",
            "Epoch[2/2], step [29400/30000], Loss:0.0000\n",
            "Epoch[2/2], step [29500/30000], Loss:0.0003\n",
            "Epoch[2/2], step [29600/30000], Loss:0.0399\n",
            "Epoch[2/2], step [29700/30000], Loss:0.0000\n",
            "Epoch[2/2], step [29800/30000], Loss:0.0000\n",
            "Epoch[2/2], step [29900/30000], Loss:0.0000\n",
            "Epoch[2/2], step [30000/30000], Loss:0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test the model\n",
        "\n",
        "#in test phase we dont need to calculate the gradients(for memory efficiency)\n",
        "\n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "\n",
        "  for images, lables in test_loader:\n",
        "    images = images.reshape(-1,28*28).to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "    #max returns(value, index)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    n_samples += labels.size(0)\n",
        "    n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "  acc = 100.0 * n_correct / n_samples\n",
        "  print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NhVwDNRh0nF",
        "outputId": "e749e30d-de18-44c8-db82-6a369beae305"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 10.02 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "slFLvP2njOe3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}